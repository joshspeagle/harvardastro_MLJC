{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is PyTorch?\n",
    "================\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "-  A replacement for NumPy to use the power of GPUs\n",
    "-  a deep learning research platform that provides maximum flexibility\n",
    "   and speed\n",
    "-  Automatic Differentiation! \n",
    "\n",
    "Getting Started\n",
    "---------------\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that\n",
    "Tensors can also be used on a GPU to accelerate computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a 5x3 matrix, uninitialized:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[                    0.0000, -36893488147419103232.0000,\n",
      "                            -0.0434],\n",
      "        [                   -0.0000,         -536871043072.0000,\n",
      "                             0.0000],\n",
      "        [        -537749094400.0000,                     0.0000,\n",
      "                 -537753288704.0000],\n",
      "        [                    0.0000,                     0.0000,\n",
      "                             0.0000],\n",
      "        [                    0.0000,                     0.0000,\n",
      "                             0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2073, 0.8341, 0.9755],\n",
      "        [0.2849, 0.7047, 0.0364],\n",
      "        [0.2763, 0.3540, 0.4225],\n",
      "        [0.1600, 0.7767, 0.0846],\n",
      "        [0.1473, 0.6171, 0.5030]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a matrix filled zeros and of dtype long:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create a tensor based on an existing tensor. These methods\n",
    "will reuse properties of the input tensor, e.g. dtype, unless\n",
    "new values are provided by user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.0182, -0.9368, -1.1277],\n",
      "        [-0.0615, -0.1775, -1.0728],\n",
      "        [-0.7097, -0.5652,  0.6762],\n",
      "        [-0.6408, -1.6329,  0.5417],\n",
      "        [ 1.2161,  0.3296, -0.2831]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get its size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>\n",
    "\n",
    "#### Operations\n",
    "\n",
    "There are multiple syntaxes for operations. In the following\n",
    "example, we will take a look at the addition operation.\n",
    "\n",
    "Addition: syntax 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5622, -0.2183, -0.3741],\n",
      "        [ 0.3867,  0.0650, -0.1987],\n",
      "        [ 0.0500,  0.0041,  0.7646],\n",
      "        [-0.1069, -1.4845,  1.0342],\n",
      "        [ 1.6213,  1.0053, -0.1239]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: syntax 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5622, -0.2183, -0.3741],\n",
      "        [ 0.3867,  0.0650, -0.1987],\n",
      "        [ 0.0500,  0.0041,  0.7646],\n",
      "        [-0.1069, -1.4845,  1.0342],\n",
      "        [ 1.6213,  1.0053, -0.1239]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: providing an output tensor as argument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6747, -0.8947,  0.7307],\n",
      "        [ 0.4895, -0.2022,  2.2190],\n",
      "        [ 0.9566, -0.5398, -0.7915],\n",
      "        [ 2.1248, -1.6050, -0.4494],\n",
      "        [ 0.5429,  2.0196,  1.5676]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: in-place\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5622, -0.2183, -0.3741],\n",
      "        [ 0.3867,  0.0650, -0.1987],\n",
      "        [ 0.0500,  0.0041,  0.7646],\n",
      "        [-0.1069, -1.4845,  1.0342],\n",
      "        [ 1.6213,  1.0053, -0.1239]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
    "    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>\n",
    "\n",
    "You can use standard NumPy-like indexing with all bells and whistles!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9368, -0.1775, -0.5652, -1.6329,  0.3296])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1676])\n",
      "0.16759651899337769\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read later:**\n",
    "\n",
    "\n",
    "  100+ Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  are described\n",
    "  `here <http://pytorch.org/docs/torch>`_.\n",
    "\n",
    "NumPy Bridge\n",
    "------------\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting a Torch Tensor to a NumPy Array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the numpy array changed in value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting NumPy Array to Torch Tensor\n",
    "\n",
    "See how changing the np array changed the Torch Tensor automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to`` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the basics kind of make sense! \n",
    "\n",
    "### Basic autograd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** the requires_grad flag. By default, this flag is set to False. If you want to do any differntiation (which we usually do), you have to set this flag to true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad) # b.grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another autograd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #neural network module\n",
    "from torch import optim #optimization module \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.2743, -0.0814, -0.4749],\n",
      "        [-0.2317, -0.2415,  0.3400]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.0570, -0.3021], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.8021619319915771\n",
      "dL/dw:  tensor([[ 0.3395, -0.3208, -0.9197],\n",
      "        [-0.8354,  0.3261,  0.4389]])\n",
      "dL/db:  tensor([-0.2025, -0.6804])\n",
      "loss after 1 pass:  1.7767481803894043\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss() #loss\n",
    "optimizer = optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "\n",
    "print(\"loss after 1 pass: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom dataset! this is a simple example but pytorch's infrastructure for custom datasets is extremely powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /Users/harshilkamdar/.torch/models/resnet18-5c106cde.pth\n",
      "100%|██████████| 46827520/46827520 [00:06<00:00, 7747737.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size()) # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together for an example of \n",
    "\n",
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.7401\n",
      "Epoch [10/60], Loss: 0.4512\n",
      "Epoch [15/60], Loss: 0.3340\n",
      "Epoch [20/60], Loss: 0.2864\n",
      "Epoch [25/60], Loss: 0.2670\n",
      "Epoch [30/60], Loss: 0.2590\n",
      "Epoch [35/60], Loss: 0.2556\n",
      "Epoch [40/60], Loss: 0.2541\n",
      "Epoch [45/60], Loss: 0.2534\n",
      "Epoch [50/60], Loss: 0.2530\n",
      "Epoch [55/60], Loss: 0.2527\n",
      "Epoch [60/60], Loss: 0.2524\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a v simple neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5134\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2569\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3941\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0982\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1544\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1251\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1723\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0840\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1020\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1115\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1584\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1350\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0939\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0858\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1173\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0652\n",
      "Epoch [3/5], Step [500/600], Loss: 0.1004\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0271\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0161\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0295\n",
      "Epoch [4/5], Step [300/600], Loss: 0.1186\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0454\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1586\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0195\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0227\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0531\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1053\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0277\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0424\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0677\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.68 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): #we don't need to compute gradients. helps with memory efficiency\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
